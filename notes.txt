rm -rf ~/.cache/torch/inductor
rm -rf /tmp/torchinductor_*
rm -rf ~/.cache/huggingface


export TORCHINDUCTOR_DISABLE=1 (WORKSSSSSSS MURCIANOOOOOO)
export TRANSFORMERS_NO_FAST_TOKENIZER=1 (ignore, attempt to run whisper with vllm but not possible)
export VLLM_DISABLED_KERNELS=MacheteLinearKernel


pytorch-triton                    3.3.1+gitc8757738
torch                             2.9.0.dev20250706+cu128
torchaudio                        2.8.0.dev20250704+cu128
torchvision                       0.24.0.dev20250706+cu128

xformers                          0.0.32+8354497.d20250707

setuptools                        79.0.1
setuptools-scm                    8.3.1

vllm                              0.9.2.dev398+g220aee902.d20250704


pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main
	-> for installing xformers



Pytorch version that works with GeForce RTX 5009 with Cuda 12.8

pip install torch==2.7.0+cu128 --index-url https://download.pytorch.org/whl/cu128

#!/bin/bash
cd /usr/share/imbox/Text2Text
/usr/local/bin/vllm serve "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4" --dtype auto --api-key fef24ab3d0a2f49a9100ae983ca35cd5e0ddb2c8f610f6930a4713bbf969f7ee --tensor-parallel-size 4 --pipeline-parallel-size 1 --gpu-memory-utilization 0.95 --host 127.0.0.1 --port 8000 2>> /usr/share/imbox/log/Text2Text/stderr.log 1>> /usr/share/imbox/log/Text2Text/stdout.log &
echo $! > /var/run/Text2Text.pid

############ STABLE VERIONS ##############################
Research suggests the NVIDIA driver version 576.88 is compatible with your RTX 5090 GPUs.

It seems likely that CUDA 12.8 is the correct version for your setup, as it matches the RTX 5090's capabilities.

The evidence leans toward using a PyTorch nightly build (e.g., torch==2.8.0.dev20250324+cu128) for
CUDA 12.8 support, given the RTX 5090's new architecture.

############ GUIDE ON TO SET UP TINY BOX ##############################
1. connect via ssh
2. sudo su
3. add docker image
4. docker ps
5. docker attach new ID of the container
6. mkdir /usr/share/imbox/Text2Text
7. cd /usr/share/imbox/Text2Text
8. pip install --pre torch==2.8.0.dev20250324+cu128 torchvision==0.22.0.dev20250325+cu128 torchaudio==2.6.0.dev20250325+cu128 --index-url https://download.pytorch.org/whl/nightly/cu128

############ START SCRIPT ##############################


#!/bin/bash
cd /usr/share/imbox/Text2Text
/usr/local/bin/vllm serve "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4" --dtype auto --api-key fef24ab3d0a2f49a9100ae983ca35cd5e0ddb2c8f610f6930a4713bbf969f7ee --tensor-parallel-size 4 --pipeline-parallel-size 1 --gpu-memory-utilization 0.95 --host 127.0.0.1 --port 8000 2>> /usr/share/imbox/log/Text2Text/stderr.log 1>> /usr/share/imbox/log/Text2Text/stdout.log &
echo $! > /var/run/Text2Text.pid

#!/bin/bash
cd /usr/share/imbox/Text2Text
vllm serve "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4" --dtype auto --api-key fef24ab3d0a2f49a9100ae983ca35cd5e0ddb2c8f610f6930a4713bbf969f7ee --tensor-parallel-size 4 --pipeline-parallel-size 1 --gpu-memory-utilization 0.95 --host 127.0.0.1 --port 8000 --max-num-seqs 16 2>> /usr/share/imbox/log/Text2Text/stderr.log 1>> /usr/share/imbox/log/Text2Text/stdout.log &
echo $! > /var/run/Text2Text.pid


############ COMANDO FINAL (que supuestamente funciona...) ##############################
vllm serve "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4" --dtype auto --api-key fef24ab3d0a2f49wa9100ae983ca35cd5e0ddb2c8f610f6930a4713bbf969f7ee --tensor-parallel-size 4 --pipeline-parallel-size 1 --gpu-memory-utilization 0.80 --host 127.0.0.1 --port 8000 --max-model-len 8192 --max-num-seqs 16 &

##############################################################################################################################
fef24ab3d0a2f49a9100ae983ca35cd5e0ddb2c8f610f6930a4713bbf969f7ee





#!/bin/bash
cd /usr/share/imbox/Text2Text
source /usr/share/imbox/Text2Text/vllm/.venv/bin/activate
/usr/share/imbox/Text2Text/vllm/.venv/bin/vllm serve  "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4" --dtype auto --api-key fef24ab3d0a2f49a9100ae983ca35cd5e0ddb2c8f610f6930a4713bbf969f7ee --tensor-parallel-size 4 --pipeline-parallel-size 1 --gpu-memory-utilization 0.85 --host 127.0.0.1 --port 8000 --max-model-len 8192 --max-num-seqs 16 2>>/usr/share/imbox/log/Text2Text/stderr.log 1>>/usr/share/imbox/log/Text2Text/stdout.log &
echo $! > /var/run/Text2Text.pid
~
